Foundations of data science
A MIT R ANA , B ASTIAN S CHNEIDER , G ERD M UND , P ENELOPE M ÜCK ,
J ONATHAN L ENNARTZ , A NNIKA TARNOWSKI & M ICHAEL N ÜSKEN

1. Exercise sheet
Handin as announced on eCampus
fits to 01

Exercise 1.1 (Computer scientist’s random variables).

(8 points)

Consider the following algorithm:

b-i

t

1. Throw a coin C ←− {0, 1}.
2. Choose X ←− [0, 1] uniformly.
3. Roll a die D ←− {1, 2, 3, 4, 5, 6}.
4. If C = 0 then
5.
Let Z ← D + X.
6. Else
7.
Let Z ← D − X.
8. Return Z

The output Z of this algorithm is a random variable.
(i) Compute its expection E(Z).

3

(ii) Compute its second moment E(Z 2 ).

4

(iii) Compute its variance var(Z).

1

Solution.

Notice

◦ E(X) =

!1

1
0 x dx = 2 ,

!1
◦ E(X 2 ) = 0 x2 dx = 31 and
"
7
◦ E(D) = d∈{1,2,3,4,5,6} d prob (D = d) = 21
6 = 2,
"
2
2
2
+52 +62
◦ E(D2 ) = d∈{1,2,3,4,5,6} x2 prob (D = d) = 1+2 +3 +4
= 91
6
6 .

Thus conditioned on C = 0 we find

E (Z C = 0) = E(D + X) = E(D) + E(X) = 4

8

Amit, Bastian, Gerd, Penelope, Jonathan, Annika & Michael

and conditioned on C = 1 we find
E (Z C = 0) = E(D − X) = E(D) − E(X) = 3.
Interludium total probability: We can always split probalities into pieces via conditional
probabilities
on
the
pieces
of
a
partition
of
the
universe:
#
If Ω = i Ωi then
$
prob (V ∈ A Ωi ) prob (Ωi )
prob (V ∈ A) =
i

This translates to expected values:
E (V ) =

$

E (V Ωi ) prob (Ωi ) .

i

b-i
t

You can easily check this by using the definitions. This is the tool to deal with case distinctions in programs in general.
(i) In the case at hand the interludium means that we simply compute
E(Z) =

1
1
E (Z C = 0) + E (Z C = 1) .
2
2

and so

E(Z) = E(D) =

7
.
2

(ii) Similarly we get

E(Z 2 ) = E((D + X)2 ) prob (C = 0) + E((D − X)2 ) prob (C = 1)
&
1%
E(D2 ) + 2 E(D) E(X) + E(X 2 )
=
2
&
1%
+ E(D2 ) − 2 E(D) E(X) + E(X 2 )
2
91 1
31
= E(D2 ) + E(X 2 ) =
+ =
.
6
3
2

(iii) Consequently, var(Z) = E(Z 2 ) − E(Z)2 = 31
2 −

% 7 &2
2

= 13
4 .

⃝

fits to 02

Exercise 1.2 (Apply Markov’s and Chebyshev’s inequalities). (12 points)

In this exercise you shall test the quality of Markov’s and Chebyshev’s
inequalities.
(i) Consider a uniform real number from the unit interval, ie. U ←− [0, 1]
uniformly chosen.

9

Foundations of data science

2

(b) Plot prob (|U − E U| ≥ c) and varc2U as functions of c.
(c) Compare and interpret.

Solution.

1

1

Markov

prob (|U − E U | ≥ c)

EU
a

var U
c2

0
0

Chebyshev

prob (U ≥ a)

1

2

a

0
0

1

2

c

t

1

(a) Plot prob (U ≥ a) and EaU as functions of a.

We can see that the Markov bound for the variable X is very loose and does not
contain much information. For example, in the interval [0, 21 ] the values of E(U )
are larger than 1, even if we know that all probabilities are naturally bounded by
1. The tail bound is also extremely loose, as even if X > 1 is impossible, we still
see a significant value of the Markov bound there.

b-i

1

The Chebychev inequality is also quite meaningless for small values of c. Even for
c close to 0.5, which is the end of the range that c can take, we still have a lot of
difference between the bound and the actual function. The tail bounds here are
much better than in the Markov case and tend to 0 very quickly.
⃝

(ii) Consider a normally distributed real number with mean 0 and variance 1, ie. X ←− N (0, 1).
(a) Plot prob (X ≥ a) and EaX as functions of a.

(b) Plot prob (|X − E X| ≥ c) and varc2X as functions of c.
(c) Compare and interpret.

Solution.

1

1

Markov

prob (|X − E X| ≥

EX
a

var X
c2

0
0

1

2

Chebyshev

prob (X ≥ a)

3

a
4

0
0

1

2

3

c
4

1
1
2

10

Amit, Bastian, Gerd, Penelope, Jonathan, Annika & Michael
The most important thing to note is that Markov’s inequality does not apply here.
This is due to the fact, that X is a random variable that can take negative values.
Markov is only applicable for non-negative random variables.
Chebychev’s inequality yields a decent bound, except for very small c.

⃝

(iii) Consider a fair die, ie. D ←− {1, 2, 3, 4, 5, 6} with uniform distribution.
1
1
2

(a) Plot prob (D ≥ a) and EaD as functions of a.

(b) Plot prob (|D − E D| ≥ c) and varc2D as functions of c.
(c) Compare and interpret.

b-i
t

Solution.
1

1

Chebyshev

prob (|D − E D| ≥ c)
var D
c2

Markov
prob (D ≥ a)
ED
a

0

0

1

2

3

4

5

6

7

a

0

0

1

2

3

4

5

6

7

c

We see again that the Markov inequality is a quite rough bound. The tail bound of
Chebychev is better, but it’s still not very good.
⃝
fits to 02

Exercise 1.3 (When is Markov sharp?).

+4

(0+4 points)

Show that for any a > 0 there exists a probability distribution such that
. Use the prob (. . .)the Markov inequality is sharp, ie. prob (X ≥ a) = E(X)
a
notation to write down the distribution explicitly.
Hint: Recall the proof from the lecture. If it helps you, restrict to the discrete setting.

Solution.

We use that prob (X ∈ I) = E(1{X∈I} ) since
E(1{X∈I} ) =

1
$
i=0

%
&
i · prob 1{X∈I} = i

%
&
= 1 · prob 1{X∈I} = 1
= prob (X ∈ I) .

Foundations of data science

11

Applied to the equation, we find that the following are equivalent:
E(X)
.
a
a · prob (X ≥ a) = E(X).
E(a · 1{X≥a} ) = E(X · 1{X≥0} ).
prob (X ≥ a) =

E(X · 1{X≥0} − a · 1{X≥a} ) = 0.

E(X · 1{0≤X<a} + (X − a) · 1{X≥a} ) = 0.
E(X · 1{0≤X<a} ) = 0 ∧ E((X − a) · 1{X≥a} ) = 0.
prob (0 < X < a) = 0

∧

prob (0 < X − a) = 0.

For the last step we use: for any set M and any random variable Z with Z ≥ 0 on M we
have that E(Z · 1M ) = 0 implies prob (Z ∈ M \ {0}) = 0. As a consequence Markov is
sharp iff prob (X ∈ {0, a}) = 1.

b-i

t

In other words, each distribution that satisfies above requirement is given by some p ∈
[0, 1] and
prob (X = a) = p, prob (X = 0) = 1 − p.
⃝

